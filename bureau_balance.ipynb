{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c61329-6847-49b7-981d-3c1f81cc59c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar sessão Spark (opcional se estiver no notebook Databricks)\n",
    "spark = SparkSession.builder.appName(\"fs_bureau_balance\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7a2957-9ed9-4cc2-b1b5-f4d5cfb0e794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs_bureau_balance = spark.table(\"podbank.raw.bureau_balance\")\n",
    "fs_bureau_balance.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ab2e85-8692-47fc-a1d3-ff040742f587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_rows = fs_bureau_balance.count()\n",
    "num_columns = len(fs_bureau_balance.columns)\n",
    "\n",
    "print(f'Quantidade de linhas: {num_rows}')\n",
    "print(f'Quantidade de variaveis (colunas): {num_columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d52ad6-5ff9-4251-bef0-5a9580b2a555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_temp_01 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    CASE WHEN MONTHS_BALANCE >= -3 THEN 1 ELSE 0 END AS U3M,\n",
    "    CASE WHEN MONTHS_BALANCE >= -6 THEN 1 ELSE 0 END AS U6M,\n",
    "    CASE WHEN MONTHS_BALANCE >= -9 THEN 1 ELSE 0 END AS U9M,\n",
    "    CASE WHEN MONTHS_BALANCE >= -12 THEN 1 ELSE 0 END AS U12M\n",
    "FROM podbank.raw.bureau_balance\n",
    "ORDER BY SK_ID_BUREAU\n",
    "\"\"\")\n",
    "df_temp_01.createOrReplaceTempView(\"df_temp_01\")\n",
    "display(df_temp_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4842817-ded3-405a-a48c-0f2523a98631",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756068431272}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_temp_02 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    SK_ID_BUREAU,\n",
    "    MONTHS_BALANCE,\n",
    "    STATUS,\n",
    "    CASE\n",
    "        WHEN STATUS = \"C\" THEN 1\n",
    "        ELSE 0\n",
    "    END AS STATUS_C,\n",
    "    CASE\n",
    "        WHEN STATUS = \"0\" THEN 1\n",
    "        ELSE 0\n",
    "    END AS STATUS_0,\n",
    "    CASE\n",
    "        WHEN STATUS = \"X\" THEN 1\n",
    "        ELSE 0\n",
    "    END AS STATUS_X,\n",
    "    CASE\n",
    "        WHEN STATUS = \"1\" THEN 1\n",
    "        ELSE 0\n",
    "    END AS STATUS_1,\n",
    "    U3M,\n",
    "    U6M,\n",
    "    U9M,\n",
    "    U12M\n",
    "FROM df_temp_01\n",
    "ORDER BY `SK_ID_BUREAU`;\n",
    "\"\"\")\n",
    "df_temp_02.createOrReplaceTempView(\"df_temp_01\")\n",
    "df_temp_02.display(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc09359c-6c54-4cb8-b637-463fa6b8b63b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round, sum, avg, max, min, when, count, lit\n",
    "\n",
    "colunas_agregacao_total = ['STATUS_C','STATUS_0','STATUS_X','STATUS_1']\n",
    "\n",
    "colunas_flags = ['U3M','U6M', 'U9M', 'U12M']\n",
    "expressoes_agregacao = []\n",
    "\n",
    "for flag in colunas_flags:\n",
    "  for coluna in colunas_agregacao_total:\n",
    "    expressoes_agregacao.append(round(count(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_TT_{coluna.upper()}_{flag.upper()}_BUREAU_BL\"))\n",
    "    expressoes_agregacao.append(round(avg(when(col(flag) == 1, col(coluna)).otherwise(lit(None))), 2).alias(f\"QT_MED_{coluna.upper()}_{flag.upper()}_BUREAU_BL\"))\n",
    "    expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MAX_{coluna.upper()}_{flag.upper()}_BUREAU_BL\"))\n",
    "    expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f\"QT_MIN_{coluna.upper()}_{flag.upper()}_BUREAU_BL\"))\n",
    "\n",
    "expressoes_agregacao = tuple(expressoes_agregacao)\n",
    "\n",
    "# Aplicar as expressões de agregação\n",
    "df_temp_03 = df_temp_02.groupBy(\"SK_ID_BUREAU\").agg(*expressoes_agregacao).orderBy(\"SK_ID_BUREAU\")\n",
    "\n",
    "# Quantidade e nome das variáveis criadas.\n",
    "nomes_cols = df_temp_03.columns\n",
    "nomes_cols_novas = nomes_cols[1:]\n",
    "print('Quantidade Total de Variáveis Criadas:', len(df_temp_03.columns) - 1)\n",
    "print('Nomes das Variáveis Criadas:', nomes_cols_novas)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Quantidade de linhas do DataFrame.\n",
    "num_rows_df = df_temp_03.count()\n",
    "\n",
    "# Quantidade de colunas do DataFrame.\n",
    "num_columns_df = len(df_temp_03.columns)\n",
    "\n",
    "# Imprimir o resultado de número de linhas e colunas.\n",
    "print(f'Quantidade de linhas do DataFrame: {num_rows_df}')\n",
    "print(f'Quantidade de colunas do DataFrame: {num_columns_df}')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Mostrando o novo DataFrame com as variáveis criadas.\n",
    "display(df_temp_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ae25cd-e81d-4324-b31c-b7a9fe7c13e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, date_format\n",
    "\n",
    "# Adicionando as colunas de data ao DataFrame.\n",
    "df_temp_04 = df_temp_03.withColumn('PK_DATREF', date_format(current_date(), 'yyyyMMdd')) \\\n",
    "                       .withColumn('PK_DAT_PROC', current_date())\n",
    "\n",
    "# Quantidade e nome das variáveis criadas.\n",
    "nomes_cols = df_temp_04.columns\n",
    "nomes_cols_novas = nomes_cols[1:-2]\n",
    "print('Quantidade Total de Variáveis Criadas:', len(df_temp_04.columns) - 3)\n",
    "print('Nomes das Variáveis Criadas:', nomes_cols_novas)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Quantidade de linhas do DataFrame.\n",
    "num_rows_df = df_temp_04.count()\n",
    "\n",
    "# Quantidade de colunas do DataFrame.\n",
    "num_columns_df = len(df_temp_04.columns)\n",
    "\n",
    "# Imprimir o resultado de número de linhas e colunas.\n",
    "print(f'Quantidade de linhas do DataFrame: {num_rows_df}')\n",
    "print(f'Quantidade de colunas do DataFrame: {num_columns_df}')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Mostrando o novo DataFrame com as variáveis criadas.\n",
    "display(df_temp_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c89b86-6c0d-4d1b-a339-4ee4916c6328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "        df_temp_04.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", True) \\\n",
    "            .saveAsTable(\"podbank.feature_store.bureau_balance\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bureau_balance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
